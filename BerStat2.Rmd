---
title: "Project 2 - TMA4300 Computer Intensive Statistical Methods"
author: "Jostein Aasteb√∏l Aanes, Frederick Nilsen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output:
  bookdown::pdf_document2:
    keep_tex: yes
    toc: no
    number_sections: false
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)        # Plotting
library(bayestestR)     # CI function
library(profvis)        # Performance analysis tool
library(latex2exp)      # Latex in plots
library(bookdown)       # Referencing figures
library(ggpubr)         # ggplot subplots
library(microbenchmark) # Testing runtime of functions
library(Matrix)         # To utilize sparse matrices

load("rain.rda")      # Dataset

knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.align = "center")

```

## Introduction

For this report, we will be looking at a given portion of the Tokyo rainfall data set. The data set contains data gathered from 1951 to 1989. For this report, the response is how many times amount of rainfall exceeded 1mm on a given day, for each of the years. Let $t$ denote the days in a year, i.e. $t=1,2,\dots,366$, then let $y_t$ be the amount of times that it rained more than 1mm on day $t$. Furthermore, let $n_t$ denote the amount of times day $t$ was recorded, which means how many years day $t$ occurred in the given timespan. This means that $n_t=39$ for all $t$ except for $t=60$, where $n_{60}=10$, since February 29th only occurs every fourth year. Lastly, let $\pi(\tau_t)$ be the probability that it rains more than 1mm on day $t$.

We assume that $y_t$ given $\tau_t$ is binomially distributed with $n_t$ trials and $\pi(\tau_t)$ probability of success for each trial. We also assume conditionall independence among the $y_t|\tau_t$ for all $t$. Furthermore, $\tau_t$ is the logit probability of exceedence. In other words:
$$
y_t|\tau_t\sim \text{Bin}(n_t,\pi(\tau_t)),\quad\pi(\tau_t)=\frac{1}{1+e^{-\tau_t}}.
$$

### Exploration of the data set

To start the data exploration, we plot the response $y_t$ as a function of $t$, which can be seen in Figure \@ref(fig:raindata). There is clearly a pattern. One can see how the different seasons affect the amount of rain. During the winter, which is for $t$ close to zero or $t$ close to 366, there is visibly less rain, compared to the summer which is around $t\in[100,250]$ in which there is considerably more rain. This makes sense as seasons affect the amount of rain. One interpretation of Figure \@ref(fig:raindata), is that if there is a lot of rain in a given period, then there will be less rain in the following period, and the other way around. Notably, one can see that there is a short period around $t=225$ where there is noticeably less rain compared to the preceding and following period where there is much rain.  

```{r raindata, fig.cap=r'($y$ plotted against $t$)'}
load(file = "rain.rda")  #Loading in the Tokyo rainfall data

#Plotting rain as a function of the day.
ggplot(rain, aes(day,n.rain)) + geom_point() +
  ggtitle(TeX(r'($y_t$ plotted against $t$)')) + xlab("t")+ 
  ylab(TeX(r'($y_t$)'))
```
Furthermore, it is of interest to get a more quantitative summary of the data. We achieve this by using the built in r function "summary()", and the result can be seen below. From the output, we see that $y_t$ has a mean of approximately $11$ and the median is $11$. The fact that the mean and median are almost equal indicates well balanced data, without any outliers.

```{r}
summary(rain)
```

## Likelihoods and conditionals
Although we now have a general understanding of our data, we would like to further explore some of its statistical properties.

### Obtaining likelihood of $y_t$ given $\pi(\tau_t)$

We wish to find $\mathcal L(\pi(\tau_t)|y_t)$. By definition
$$
\mathcal L(\pi(\pmb\tau)|\pmb y)=f(y_1,y_2,...,y_{366}|\pi(\tau_1), \pi(\tau_2),...,\pi(\tau_{366}))
$$
where $f$ is the joint conditional distribution. Now, we have that $f(y_1,...,y_{366}|\pi(\tau_1),...,\pi(\tau_{366}))=f(y_1|\pi(\tau_1),...,\pi(\tau_{366}))\cdot f(y_2,...,y_{366}|y_1,\pi(\tau_1),...,\pi(\tau_{366}))$. Since $y_1$ is conditionally independent all $\pi(\tau_j)$ for $j\neq 1$, we get that $f(y_1|\pi(\tau_1),...,\pi(\tau_{366})=f(y_1|\pi(\tau_1))$. In addition, all $y_j$ where $j\neq1$ are conditionally independent $y_1$ and $\pi(\tau_1)$ given $\pi(\tau_j)$, thus $f(y_2,...,y_{366}|y_1,\pi(\tau_1),...,\pi(\tau_{366}))=f(y_2,...,y_{366}|\pi(\tau_2),...,\pi(\tau_{366}))$. Using the same argument for $t=2,...,366$, one gets that the joint distribution is equal to
$$
f(y_1,y_2,...,y_{366}|\pi(\tau_1), \pi(\tau_2),...,\pi(\tau_{366}))=\prod_{i=1}^{366}f(y_i|\pi(\tau_i)).
$$
Hence
$$
\mathcal L(\pi(\pmb\tau)|\pmb y)=\prod_{i=1}^{366}\mathcal L(\pi(\tau_i)|y_i)=\prod_{i=1}^{366}f(y_i|\pi(\tau_i))
$$
which means that
$$
\mathcal L(\pi(\tau_i)|y_i)=f(y_i|\pi(\tau_i))
$$
where $f(y_i|\pi(\tau_i))=\binom{n_i}{y_i}\pi(\tau_i)^{y_i}(1-\pi(\tau_i))^{n_i-y_i}$. 

### Establishing the Bayesian hierarchical model
Now, we will apply a Bayesian hierarchical model to the dataset, using a Random walk of order 1 to model the trend as $\tau_t\sim\tau_{t-1}+u_t$ where $u_t\sim\mathcal N(0,\sigma_u^2)$ are identically and independently distributed. This in turn implies that $p(\tau_t|\sigma_u^2)\sim\mathcal N(\tau_{t-1},\sigma_u^2)$. We have that 
$$
p(\pmb\tau|\sigma_u^2)=\prod_{t=2}^{T}\frac{1}{\sigma_u^2}e^{-\frac{1}{2\sigma_u^2}(\tau_t-\tau_{t-1})^2}
$$
We impose an inverse gamma prior on $\sigma_u^2$ as follows
$$
p(\sigma_{u}^2)=\frac{\beta^\alpha}{\Gamma(\alpha)}\left(\frac{1}{\sigma_u^2}\right)^{\alpha+1}e^{-\frac{\beta}{\sigma_u^2}}
$$
We also introduce the notation $\pmb y=(y_1,...,y_{366})^T$, $\pmb\tau=(\tau_1,...,\tau_{366})^T$ and $\pmb\pi=(\pi(\tau_1),...,\pi(\tau_{366}))^T$.


### Finding the conditional $p(\sigma_u^{2}|\pmb y, \pmb \tau)$
We now wish to find the conditional $p(\sigma_u^2|\pmb y,\pmb\tau)$.

Firstly
$$
p(\sigma_u^{2}|\pmb y, \pmb \tau)\propto p(\pmb y|\sigma_u^2, \pmb\tau)\cdot p(\pmb\tau|\sigma_u^2)\cdot p(\sigma_u^2).
$$
Now, $\pmb y$ is conditionally independent $\sigma_u^2$ given $\pmb\tau$. Thus, $p(\pmb y|\sigma_u^2, \pmb\tau)=p(\pmb y| \pmb\tau)$. We know that $p(\pmb y| \pmb\tau)$ does not include any terms with $\sigma_u^2$. Therefore, we get
$$
p(\sigma_u^{2}|\pmb y, \pmb \tau)\propto p(\pmb\tau|\sigma_u^2)\cdot p(\sigma_u^2).
$$


This becomes in turn
$$
p(\sigma_u^{2}|\pmb y, \pmb \tau)\propto\prod_{t=2}^T\frac{1}{\sigma_{u}}\exp\left(-\frac{1}{2\sigma_u^2}(\tau_t-\tau_{t-1})^2\right)\times \frac{\beta^{\alpha}}{\Gamma(\alpha)}\left(\frac{1}{\sigma_u^2}\right)^{\alpha+1}\exp\left(-\frac{\beta}{\sigma_u^2}\right)
$$

Disregarding the constant $\beta^\alpha/\Gamma(\alpha)$, this simplifies to
$$
\left(\frac{1}{\sigma_u}\right)^{T-1} \exp\left(\sum_{t=2}^T\frac{-1}{2\sigma_u^2}(\tau_t-\tau_{t-1})^2+\frac{-\beta}{\sigma_u^2}\right) \left(\frac{1}{\sigma_u^2}\right)^{\alpha+1}
$$
$$
=(\sigma_u^2)^{-(\alpha+\frac{T-1}{2})-1} \exp\left(\frac{-(\beta+\frac12 \sum_{t=2}^T(\tau_t-\tau_{t-1})^2)}{\sigma_u^2}\right)
$$

Which is the pdf of an inverse gamma distribution with parameters
$$
\alpha'=\alpha+\frac{T-1}{2},\quad \beta'=\beta+\frac12 \sum_{t=2}^T(\tau_t-\tau_{t-1})^2)
$$

## Sampling with a Monte Carlo Markov Chain (MCMC)

With our relations derived so far, we have almost everything needed to implement a MCMC sampler, using the Metropolis-Hastings
algorithm to sample $\pmb\tau$ and Gibbs algorithm to sample $\sigma_u^2$. All we need before the implementing a MCMC sampler, is an expression for the acceptance probability.

### Acceptance probability for MH-step

Let $\mathcal I\subseteq \{1,\dots,366\}$ be a set of time indices, and let $\pmb\tau_{\mathcal I}'$ be proposed values for $\pmb\tau_{\mathcal I}$. Furthermore, $\pmb\tau_{-\mathcal I}=\pmb\tau_{\{1,\dots,366\}\backslash \mathcal I }$ be a subset of $\pmb\tau$ which includes all $\tau_t$ except for those with indices in $\mathcal I$. We have that the prior proposal distribution is
$$
Q(\pmb\tau_{\mathcal I}'|\pmb\tau_{-\mathcal I}, \sigma_u^2,\pmb y) = p(\pmb\tau_{\mathcal I}'|\pmb\tau_{-\mathcal I}, \sigma_u^2).
$$
We consider it known that the acceptance probability is given by
$$
\alpha = \min\left(1, \frac{p(\pmb\tau_{\mathcal I}' | \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)}{p(\pmb\tau_\mathcal I | \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)} \frac{Q(\pmb\tau_\mathcal I | \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)}{Q(\pmb\tau_\mathcal I' | \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)}\right)
$$
It is preferable to simplify the last expression as much as possible.

The ratio including the prior proposal distribution $Q(\cdot)$ becomes
$$
\frac{p(\pmb\tau_\mathcal I | \pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb\tau_\mathcal I' | \pmb\tau_{-\mathcal I}, \sigma_u^2)}.
$$
Considering now the numerator for the first factor,
$$
p(\pmb\tau_\mathcal I' | \pmb\tau_{-\mathcal I},\sigma_u^2, \pmb y)=\frac{p(\pmb\tau_\mathcal I', \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)}{p(\pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y)} = \frac{p(\pmb y | \pmb\tau_\mathcal I ', \pmb\tau_{-\mathcal I}, \sigma_u^2)\cdot p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I},\sigma_u^2)\cdot p(\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y|\pmb\tau_{-\mathcal I}, \sigma_u^2)\cdot p(\pmb\tau_{-\mathcal I},\sigma_u^2)}
$$
$$
=\frac{p(\pmb y|\pmb\tau_\mathcal I', \pmb\tau_{-\mathcal I})\cdot p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2)} =\frac{p(\pmb y_\mathcal I|\pmb\tau_\mathcal I') \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I})\cdot p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2)},
$$
where the last relations holds since we assume conditional independence among all $y_t|\tau_t$. Similarly for the denominator we will end up with
$$
p(\pmb\tau_\mathcal I | \pmb\tau_{-\mathcal I}, \sigma_u^2, \pmb y) = \frac{p(\pmb y_\mathcal I|\pmb\tau_\mathcal I) \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I})\cdot p(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2)}
$$

Taking the divison allows for several cancellations

$$
\frac{p(\pmb y_{\mathcal I}|\pmb\tau_\mathcal I') \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I})\cdot p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)\qquad p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2)}{p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2) \qquad p(\pmb y_I|\pmb\tau_\mathcal I) \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I})\cdot p(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I}, \sigma_u^2)} = \frac{p(\pmb y_I|\pmb\tau_\mathcal I') \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I}) \cdot p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2) \cdot p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y_I|\tau_\mathcal I) \cdot p(\pmb y_{-\mathcal I} |\pmb\tau_{-\mathcal I})\cdot p(\pmb y|\pmb\tau_{-\mathcal I},\sigma_u^2)\cdot p(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I}, \sigma_u^2)}
$$
$$
=\frac{p(\pmb y_I|\pmb\tau_\mathcal I') p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y_I|\pmb\tau_\mathcal I)p(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I}, \sigma_u^2)}
$$
Multiplying this with $\frac{Q(\pmb\tau_\mathcal I |\dots)}{Q(\pmb\tau_\mathcal I' | \dots)}$, the final factors cancel out:

$$
\frac{p(\pmb y_I|\pmb\tau_\mathcal I') p(\pmb\tau_\mathcal I'|\pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb y_I|\pmb\tau_\mathcal I)p(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I}, \sigma_u^2)} \cdot \frac{p(\pmb\tau_\mathcal I | \pmb\tau_{-\mathcal I}, \sigma_u^2)}{p(\pmb\tau_\mathcal I' | \pmb\tau_{-\mathcal I}, \sigma_u^2)} = \frac{p(\pmb y_{\mathcal I} | \pmb\tau_I')}{p(\pmb y_{\mathcal I}|\pmb\tau_I)}.
$$


Thus, we get that the acceptance probability is given by
$$
\alpha = \min\left(1, \frac{p(\pmb y_{\mathcal I} | \pmb\tau_I')}{p(\pmb y_{\mathcal I}|\pmb\tau_I)}\right)
$$
Now, we find an expression for the acceptance probability through logscaling.
$$
\alpha = \min \left\{1,\frac{\prod_{i\in I}\pi(\tau_i')^{y_i}(1-\pi(\tau_i'))^{n_i-y_i}}{\prod_{i\in I}\pi(\tau_i)^{y_i}(1-\pi(\tau_i))^{n_i-y_i}}\right\}
$$
We have that
$$
\ln\left(\prod_{i\in I}\pi(\tau_i')^{y_i}(1-\pi(\tau_i'))^{n_i-y_i}\right)=\sum_{i\in I}y_i\tau_i'+n_i\ln(1-\pi(\tau_i')).
$$
Thus, we get that
$$
\alpha = \min \left(1, \exp\left(\sum_{i\in I}y_i(\tau_i'-\tau_i)+n_i(\ln(1-\pi(\tau_i'))-\ln(1-\pi(\tau_i)))\right)\right)
$$
Which reduces to
$$
\alpha = \min\left(1, \exp\left(\sum_{i\in \mathcal I}y_i(\tau_i'-\tau_i)+n_i\log\left(\frac{1+e^{\tau_i}}{1+e^{\tau_i'}}\right)\right) \right)
$$
This will be our expression for the acceptance probability. The expression is used since it is less computationally intensive than calculating two binomial densities, and finding the ratio between them. A comparison between the newly obtained expression and the initial expression is shown in Figure \@ref(fig:binom-test), where the optimized expression clearly outperforms the standard expression.

```{r binom-test, echo=FALSE, fig.cap=r'(Distribution plot of the computational time for the original expression compared to the optimized expression. Plot is made using the R-package microbenchmark.)'}

binom_timesave <- function(testing_size){
  expit <- function(tau){return( 1 / (1+exp(-tau )))}
  tau_prime <- rnorm(366)
  tau <- rnorm(366)
  mbm <- microbenchmark(
    "Original expression" = {
      for(i in 1:testing_size){
        b <- (dbinom(rain$n.rain, rain$n.years, expit(tau_prime))/dbinom(rain$n.rain, rain$n.years, expit(tau)))
      }
    },
    "Optimized expression" = {
      for(i in 1:testing_size){
        b <- exp(rain$n.rain * (tau_prime - tau)) * exp(rain$n.years * log((1+exp(tau))/(1+exp(tau_prime))))
      }
    })
  autoplot(mbm)
  
}
binom_timesave(500)

```

### Deriving precision matrix $Q=Prec(\pmb\tau|\sigma_u^2)$ 
We have that 
$$
p(\pmb\tau|\sigma_u^2)=\prod_{t=2}^{T}\frac{1}{\sigma_u}\exp\left(-\frac{1}{2\sigma_u^2}(\tau_t-\tau_{t-1})\right)
$$
$$
=\left(\frac{1}{\sigma_u}\right)^{T-1}\exp\left(-\frac{1}{2\sigma_u^2}\sum_{t=2}^T(\tau_t-\tau_{t-1})\right)=\left(\frac{1}{\sigma_u}\right)^{T-1}\exp\left(-\frac12 \pmb\tau Q\pmb\tau^t\right)
$$
where $\pmb\tau^t$ means the transposed of $\pmb\tau$. Here, one has that
$$
Q = \frac{1}{\sigma_u^2}\begin{bmatrix}
1 & -1 & 0 & \dots & 0\\
-1 & 2 & -1 & \dots & 0\\
0 & -1 & 2 & -1 & \vdots \\
\vdots & & \ddots & \ddots \\
0 & \dots & 0 & -1 & 1
\end{bmatrix}
$$
For a single-site MCMC algorithm one then gets that the mean value $\mu_t$ for $\tau_t|\sigma_u^2$ is given by
$$
\mu_t = \begin{cases}
\tau_2,\quad t=1\\
\frac12(\tau_{t-1}+\tau_{t+2}),\;\; t\neq1\;\; \text{and}\;\;t\neq T\\
\tau_{T-1},\;\; t=T
\end{cases}
$$
and that the variance is
$$
\begin{cases}
\sigma_u^2,\quad t=1\\
\frac12\sigma_u^2,\;\; t\neq1\;\; \text{and}\;\;t\neq T\\
\sigma_u^2,\;\; t=T.
\end{cases}
$$

We can now start by determining initial values for $\pmb \tau$. Since our Bayesian hierarchical model uses a random walk of order 1, all $\tau_i \sim \mathcal N(\tau_{i-1}, \sigma_u^2)$. We initialize all $\tau_i$ as $\mathcal N(0, \widehat{\sigma_u^2})$, where $\widehat{\sigma_u^2}$ is estimated from the data. We find be empirical variance of the logit of $\widehat{\pi(y/n)}=\texttt{n.rain/n.years}$ as follows
```{r inital values}
empirical_pi = rain$n.rain/rain$n.years
sigma_0_est = var(log(empirical_pi)/(1-log(empirical_pi)), na.rm = TRUE)
```
Running the code above results in $\widehat{\sigma_u^2} = `r round(sigma_0_est,4)`$, so that a reasonable initial value is 0.007 for the variance. Thus, we initialize $\pmb \tau_t^{(0)} \sim \mathcal N(0, 0.007), \forall \; t$ for iteration 0.

### Single-site MCMC implementation

Before implementing the MCMC algorithm, we define some helper functions shown below.

```{r helper functions}

rgamma_inv <- function(n, alpha, beta){
  # Inverse gamma sampler. Simply samples from Gamma-distribution
  # and returns its reciprocal
  return(1/rgamma(n, shape=alpha, rate=beta))
}

expit <- function(tau){
  # Expit / inverse logit function
  return( 1 / (1+exp(-tau )))
}

```

One detail to keep in mind is that we specify the `rate` as $\beta$ when sampling from the gamma distribution. This is since the reciprocal of a gamma distribution with *scale* $\beta$ corresponds to an inverse gamma distribution with *rate* $\beta$. We want the inverse gamma prior to have about 95% of its density between $0.01$ and $0.25$. We can determine the correct sampling method by plotting these histograms as shown in Figure \@ref(fig:inv-gamma-sampling-result).

```{r inv-gamma-sampling-result, fig.cap=r'(Comparison of histograms for inverse gamma using either rate or scale as $\beta$. From the x-axis, we clearly see that the rate parameter produces the wanted results. )'}
par(mfrow=c(1,2))
samps_rate = 1/rgamma(5000, shape=2, rate=0.05)
samps_scale = 1/rgamma(5000, shape=2, scale=0.05)
hist(samps_scale, breaks=1000, xlim=c(0,60), main=TeX("$\\beta$ as scale"))
hist(samps_rate, breaks=500, xlim=c(0,0.5), main=TeX("$\\beta$ as rate"))
```

We may now implement the Metropolis-Hastings step. This step consists of, for each $t$, sampling the proposal $\tau_t'$ with mean and variance as previously described. If this $\tau_t'$ is such that a sampled $u \sim$ Unif$(0,1)$ is *less than* the acceptance probability $\alpha(\tau_t' | \tau_t, \sigma_u^2, \pmb y)$, we accept the step, i.e. $\tau_t \leftarrow \tau_t'$. We also store the acceptance rate for later analysis.

```{r mh_step}
mh_step <- function(curr_tau, sigma_u, k){
  # Metropolis-Hastings step for MCMC sampler
  # tau     : Matrix storing each sample value for each iteration
  # sigma_u : Standard deviation of the prior
  # k       : Iteration number, 1 <= k <= 50 000
  # returns : list of new tau and acceptance rate for this iteration
  
  tau_size = length(curr_tau) # = 366
  accepted = 0
  
  normal_samples = rnorm(tau_size)
  uniform_samples = runif(tau_size)
  
  # First tau (t=1)
  first_proposal = curr_tau[2] + sigma_u * normal_samples[1]
  # Get acceptance probability
  acceptance_probability = exp(rain$n.rain[1] * (first_proposal - curr_tau[1])+
                                 rain$n.years[1] * log((1+exp(curr_tau[1]))/(
                                   1+exp(first_proposal))))
  # Update tau if acceptance criteria is satisfied
  curr_tau[1] = ifelse(uniform_samples[1] < acceptance_probability,
                       first_proposal, curr_tau[1])
  accepted = accepted + ifelse(uniform_samples[1] < acceptance_probability, 1,0)
  
  # Do the same for all t except first and last (t != 1, t != 366)
  for (k in 2:(tau_size - 1)) {
    proposal = 1/2 *(curr_tau[k - 1] + curr_tau[k + 1]) + 
      normal_samples[k] * sigma_u/sqrt(2)
    acceptance_probability = exp(rain$n.rain[k] * (proposal - curr_tau[k]) +
                                   rain$n.years[k] * log((1+exp(curr_tau[k]))/(
                                     1+exp(proposal))))
    curr_tau[k] = ifelse(uniform_samples[k] < acceptance_probability,
                         proposal, curr_tau[k])
    accepted = accepted + ifelse(uniform_samples[k] < acceptance_probability,
                                 1, 0)
  }
  
  # Same procedure for last tau (t=366)
  last_proposal = curr_tau[tau_size - 1] + sigma_u * normal_samples[tau_size]
  acceptance_probability = exp(rain$n.rain[tau_size] * (last_proposal -
                                                          curr_tau[tau_size]) +
                                 rain$n.years[tau_size] * log(
                                   (1+exp(curr_tau[k]))/(1+exp(last_proposal))
                                   )
                               ) 
  curr_tau[tau_size] = ifelse(uniform_samples[tau_size]< acceptance_probability,
                              last_proposal, curr_tau[tau_size])
  accepted = accepted +ifelse(uniform_samples[tau_size]< acceptance_probability,
                              1, 0)
  
  return(list(tau = curr_tau, accept = (accepted/tau_size)))
}

```

We can now apply all the defined functions into a complete single-site MCMC sampler:

```{r sample_mcmc}
sample_mcmc <- function(tau_0, alpha, beta, nsteps=50000){
  # MCMC Sampling algorithm for Tokyo rainfall dataset
  # tau_0   : 366-length vector of initial tau values
  # alpha   : Shape parameter for sigma_u^2
  # beta    : Scale parameter for sigma_u^2
  # nsteps  : Number of MCMC iterations, defaults to 50.000

  # Intializations
  start = proc.time()[3] # To measure time
  tau = array(c(0), dim=c(length(tau_0), nsteps))
  current_tau = tau_0 # Fill initial values
  sigma_u = sqrt(rgamma_inv(1, alpha=alpha, beta=beta))
  accepted = 0
  variances = c()
  # Progress bar initialization
  pb = txtProgressBar(min = 0, max = nsteps, initial = 1)
  
  # Iterate nstep times
  for(k in 1:nsteps){
    setTxtProgressBar(pb,k) # Update progress bar
    
    # MH step:
    mh_res = mh_step(current_tau, sigma_u, k)
    current_tau = mh_res$tau # Updates main sample matrix
    tau[, k] = current_tau
    accepted = accepted + mh_res$accept
    
    # Gibbs step:
    variances[k] = rgamma_inv(1, alpha=alpha + (length(current_tau)-1)/2,
                            beta=beta + 0.5 * sum(diff(current_tau)^2))
    sigma_u = sqrt(variances[k])
    # Note: Variances are stored to list in each iteration
    #       so we can do traceplots on variance

  }
  close(pb)
  
  # Return tau samples, variance samples, runtime in seconds and acceptance rate
  res = list(
    tau        = tau,
    runtime    = proc.time()[3]-start,
    sigma_u    = variances,
    acceptance = accepted/nsteps
    )
  
  return(res)
}
result = sample_mcmc(tau_0=rnorm(366, 0, sqrt(0.007)),
                     alpha=2, beta=0.05, nsteps=50000)

```

This run had a runtime of `r round(result$runtime,3)` seconds with acceptance rate `r round(result$acceptance, 3)`.

We now wish to determine the performance of the sampler, and introduce some evaluation functions. In particular, we would like to plot trace plots of certain days and the sampled variance, i.e. the evolution of the samples for each iteration. We also implement plots of the histograms with 95 % credible intervals, in addition to an autocorrelation plot.

These functions are implemented below.

```{r convergence_diagnostics}
# Methods to remove burn-in from results
remove_burnin_tau <- function(tau){
  burnin = 500
  return(tail(tau[, ], n=50000-burnin))
}
remove_burnin_sigma <- function(sigma){
  burnin = 500
  return(tail(sigma, n=50000-burnin))
}

traceplot <- function(result, day=NA, xlim=NULL){
  # Plots trace plot of MCMC results
  # result : Large list containing values for both tau and sigma_u^2
  # day    : int specifying day (when plotting tau) or NA (plotting sigma_u^2)
  # xlim   : Specifies limits to plot trace plot within, defaults to [0,50000]
  
  if(is.na(day)){ # Sigma_u^2
    plot(result$sigma_u, type="l", xlab="Iteration number", xlim=xlim,
         ylab="", main=TeX(r'(Trace plot of $\sigma_u^2$)'))
  }
  else{ # tau(t=day)
    plot(result$tau[day,], type="l", xlab="Iteration number", xlim=xlim,
         ylab="", main=TeX(sprintf(
           r'(Trace plot of $\tau$ at $t = %d$)', day))) 
  }
}

plot_hist <- function(result, day=NA){
  # Plots histogram of MCMC results
  # result : Large list containing values for both tau and sigma_u^2
  # day    : int specifying day (when plotting tau) or NA (plotting sigma_u^2)
  # returns: ggplot object. Returns so it can be passed to subplot method
  
  # First, remove burn-in
  result$tau = remove_burnin_tau(result$tau)
  remove_burnin_sigma = remove_burnin_sigma(result$sigma_u)
  
  # Compute credible interval
  ci_res = if(is.na(day)) ci(result$sigma_u, ci=0.95) else
    ci(result$tau[day, ], ci=0.95)
  # Plotting parameters used by ggplot, common for all histograms
  plot_params <- list(
    geom_vline(aes(xintercept = ci_res$CI_low, color="95% credible interval")),
    geom_vline(xintercept = ci_res$CI_high, color="red"),
    theme(legend.position = "top")
  )
  
  if(is.na(day)){ # Sigma_u^2
    df = as.data.frame(result$sigma_u)
    return(ggplot(data=df, aes(x=result$sigma_u)) + geom_histogram(
      color="black", alpha=0.4, binwidth = 1e-3) + plot_params +
      labs(x=TeX("$\\sigma_u^2$"), color=""))
  }
  else{ # tau(t=day)
    df = as.data.frame(result$tau[day, ])
    return(ggplot(data=df, aes(x=result$tau[day, ])) + geom_histogram(
      color="black", alpha=0.4, binwidth = 0.04) + plot_params +
      labs(x=TeX(paste("$\\tau_t$, $t=", day)), color=""))
  }
}

compare_to_data <- function(result, plotMean=TRUE){
  # Compares pi(tau_t) to empirical pi(tau_t) for t = 1, ..., 366
  # result   : Large list as in other plotting functions
  # plotMean : bool determining if we want the mean (default) or just
  #            the last iteration
  #
  
  # First, remove burn-in
  result$tau = remove_burnin_tau(result$tau)
  remove_burnin_sigma = remove_burnin_sigma(result$sigma_u)
  
  # Plots the probability of rain alongside the sampled values for each day.
  pi_data = rain$n.rain/rain$n.years
  pi_mcmc = if(plotMean) expit(rowMeans(result$tau)) else expit(
    tail(t(result$tau), n=1)[,]
    )
  ci_low = c()
  ci_high = c()
  for(i in 1:length(result$tau[, 1])){
    ci_all = ci(expit(result$tau[i,]), ci=0.95)
    ci_low[i] = ci_all$CI_low
    ci_high[i] = ci_all$CI_high
  }
  plot(pi_data, type="l", col="grey", ylim=c(0,1))
  lines(pi_mcmc, type="l", col="red")
  lines(ci_low, type="l", col="blue")
  lines(ci_high, type="l", col="blue")
  legend('topright', lty=1, cex=0.8, col=c("grey", "red", "blue"),
        legend=c("True observations","MCMC predictions","95% credible interval")
  )
}
```

We choose to look at the results for $\sigma_u^2$, and $\tau_t$ for the days $1, 201$, and $366$. Figure \@ref(fig:result-traceplots) shows the trace plots of $\sigma_u^2$ and each of the three $\tau$ for all $50 000$ samples. We see some burn-in from most plot at the very start, but other than that we observe stationary noise for all cases.

```{r result-traceplots, echo=FALSE, fig.cap=r'(Trace plots of $\sigma_u^2$ and $\tau_t$ for $t=1, 201,366$ for all iterations. Although there is some burn-in deviations for the earliest iterations, they all generally look like white noise, which is the intended behavior.)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1.8, 1.5, 0.5))
traceplot(result)
traceplot(result, day=1)
traceplot(result, day=201)
traceplot(result, day=366)
```

If we look at the first $1000$ iterations, we are able to easier determine around what values we can expect burn-in. This is done in Figure \@ref(fig:result-traceplots-first1000). The burn-in seem to only be in around $[0,200]$. Thus, for good measure, a burn-in of about $500$ should be more than enough. For the other plots and analyses, we exclude the first 500 iterations.

```{r result-traceplots-first1000, echo=FALSE, fig.cap=r'(Trace plots of the same parameters for the 1000 first values. We see clear burn-in around the 100 first iterations, but the rest seem like noise.)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1.8, 1.5, 0.5))
plot_lim = c(0,1000)
traceplot(result, xlim=plot_lim)
traceplot(result, day=1, xlim=plot_lim)
traceplot(result, day=201, xlim=plot_lim)
traceplot(result, day=366, xlim=plot_lim)

```

The histogram plots for the same four values are given in Figure \@ref(fig:result-histograms). $\sigma_u^2$ seem to follow the density of an inverse gamma distribution, and the $\tau_t$ follow a normal distribution as expected. We can also compute the expit of the centers from the $\tau$ densities and compare them to Figure \@ref(fig:raindata).
Doing so reveals smaller probabilities for exceedence at $t=1, 366$ and a higher probability for exceedence at $t=201$. This coincides well with Figure \@ref(fig:raindata), seeing as day 201 experiences more rain than the start and end of the year.

```{r result-histograms, echo=FALSE, fig.cap=r'(Histograms of $\sigma_u^2$ and $\tau_t$ for $t=1, 201,366$ with their corresponding 95\% credible intervals.)'}

p1 = plot_hist(result)
p2 = plot_hist(result, 1)
p3 = plot_hist(result, 201)
p4 = plot_hist(result, 366)
pdf(NULL) # Fixes blank page bug in ggpubr
res <- ggarrange(p1,p2,p3,p4, ncol=2, nrow=2, common.legend=TRUE, legend="top")
x <- dev.off() # Fixes blank page bug in ggpubr
res
```

We would like to also compare the MCMC predictions to the empirical ratio `n.rain/n.years`, which  is done in Figure \@ref(fig:result-comparison). The resulting plot shows that the 95% credible interval captures the true values for most days with few exceptions. Generally, the estimated distribution from the MCMC is considerably less jagged than the empirical data, which indicates that it captures the trend and can be less prone to noise.

```{r result-comparison, echo=FALSE, fig.cap=r'(Comparison between of true probabilities versus the sampled $\pi(\tau_t)$ for each day. The samples represent the mean of all iterations, excluding burn-in.)'}
compare_to_data(result, plotMean = TRUE)
```

Lastly, we also plot the autocorrelation of the parameters as a function of lag in Figure \@ref(fig:acf-plotting). 

```{r acf-plotting, echo=FALSE, fig.cap=r'(Autocorrelation plot of the four parameters as a function of lag.)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1, 1, 2.1))
tau_no_burnin <- remove_burnin_tau(result$tau)
sigma_no_burnin <- remove_burnin_sigma(result$sigma_u)
acf(sigma_no_burnin)
mtext(TeX(r'($\sigma_u^2$)'), side=3, line=-3, outer=TRUE, adj=0.2)
acf(tau_no_burnin[1,])
mtext(TeX(r'($\tau_1$)'), side=3, line=-3, outer=TRUE, adj=0.75)
acf(tau_no_burnin[206,])
mtext(TeX(r'($\tau_{206}$)'), side=3, line=-16, outer=TRUE, adj=0.2)
acf(tau_no_burnin[366,])
mtext(TeX(r'($\tau_{366}$)'), side=3, line=-16, outer=TRUE, adj=0.75)
```
These plots show that there is strong correlation in the iterates for both $\sigma_u^2$ and all three values of $\tau_t$.

### MCMC blocking preliminaries

Although the single-site MCMC provided promising results, it could be of interest to see if we can create a blocking algorithm to further improve performance and/or the predictions. 

Consider the partionining of $\pmb\tau= [\pmb\tau_\mathcal I,\pmb\tau_{-\mathcal I}]^T$, where $\mathcal I$ represents the index set as previously described. This is multivariate normally distributed as
$$
\pmb\tau = [\pmb\tau_\mathcal I,\pmb\tau_{-\mathcal I}]^T\sim MVN\left(\left[\pmb\mu_I,\pmb\mu_{-I}\right]^T,\left(\text{Prec}(\pmb\tau | \sigma_u^2)\right)^{-1}\right),
$$
where $\text{Prec}(\pmb\tau | \sigma_u^2)$ is the already defined precision matrix. We have that
$$
\pmb\tau_{\mathcal I}'\sim f(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I})
$$
where $\pmb\tau_{\mathcal I}'$ is the proposed value for $\pmb\tau_{\mathcal I}$.
Since the Bayesian hierarchical model uses a random walk of order one, if $\mathcal I = \{a, a+1,\dots,b-1,b\}$, then $\pmb\tau_{\mathcal I}'\sim f(\pmb\tau_\mathcal I|\pmb\tau_{-\mathcal I})$ becomes $\pmb\tau_{\mathcal I}'\sim f(\pmb\tau_{\mathcal I}|\tau_{a-1},\tau_{b+1})$. Let $M$ be the length of $\mathcal I$. 

Furthermore:
$$
\pmb\mu_{\mathcal I|-\mathcal I}=-Q_{\mathcal{I,I}}^{-1}Q_{\mathcal I,-\mathcal I}\pmb\tau_{-\mathcal I}.
$$

$$
Q_{\mathcal I|-\mathcal I}=Q_{\mathcal{I,I}}
$$

We have three different cases to consider: $a = 1$, $a\neq1,\;b\neq366$ and $b=366$.
When $a=1$ we have that 
$$
Q_{\mathcal I\mathcal  I} = \frac{1}{\sigma_u^2}\begin{bmatrix}
1 & -1 & 0 & \dots & 0\\
-1 & 2 & -1 & \dots & 0\\
0 & -1 & 2 & -1 & \vdots \\
\vdots & & \ddots & \ddots
\end{bmatrix}
$$

Then $Q_{\mathcal I, -\mathcal I}$ has dimensions $M\times366-M$. In addition, $Q_{\mathcal I, -\mathcal I}$ is zero everywhere except for the element at its $M$'th row and first column, which is element $q_{M, M+1}=-1$, which is at row $M$ and column $M+1$ in the full precision matrix $Q$. Let $Q_{\mathcal I\mathcal  I}^{-1}[,M]$ be column $M$ of $Q_{\mathcal I\mathcal  I}^{-1}$, we then have that
$$
\pmb\mu_{\mathcal I|-\mathcal I}= Q_{\mathcal I\mathcal  I}^{-1}[,M]\tau_{(b+1)}
$$


When neither $a=1$ or $b=366$, the precision matrix becomes
$$
Q_{\mathcal I\mathcal  I} = \frac{1}{\sigma_u^2}\begin{bmatrix}
2 & -1 & 0 & \dots & 0\\
-1 & 2 & -1 & \dots & 0\\
0 & -1 & 2 & -1 & \vdots \\
\vdots & & \ddots & \ddots
\end{bmatrix}
$$
Here $Q_{\mathcal I, -\mathcal I}$ will be zero everywhere except for at row 1, column $a-1$ where it is -1 and row $b$, column $a+1$ where it is -1. This in turn implies that
$$
\pmb\mu_{\mathcal I|-\mathcal I}= (Q_{\mathcal I\mathcal  I}^{-1}[, 1], Q_{\mathcal I\mathcal  I}^{-1}[, M])\cdot(\tau_{a-1},\tau_{b+1})^T
$$

Lastly, when $b=366$
$$
Q_{\mathcal I\mathcal  I} = \frac{1}{\sigma_u^2}\begin{bmatrix}
2 & -1 & 0 & \dots & 0\\
-1 & 2 & -1 & \dots & 0\\
0 & -1 & 2 & -1 & \vdots \\
\vdots & & \ddots & \ddots & -1 \\
0 & \dots & 0 & -1 & 1
\end{bmatrix}
$$

Here $Q_{\mathcal I, -\mathcal I}$ is zero everywhere except for at row 1, column $a-1$ where it is -1. Hence
$$
\pmb\mu_{\mathcal I|-\mathcal I}= Q_{\mathcal I\mathcal  I}^{-1}[, 1]\tau_{a-1}
$$

### Precomputiations
Before the actual MCMC blocking algorithm is run, we perform some precomputations to save time. First of all, we precompute $\pmb\mu_{\mathcal I|-\mathcal I}$ for the three different cases shown above. Also, we compute the Cholesky decompositions of the three different $Q_{\mathcal I \mathcal I}^{-1}$ matrices. 

We use this when generating the samples $\pmb\tau_{\mathcal I}'$. First, we draw samples from a standard normal distribution. Then we utilize the fact that if $x\sim \mathcal N(0, I)$ then $y=\mu+Lx\sim\mathcal N(\mu, LL^T)$. For our case, $\mu$ will be the three different $\pmb\mu_{\mathcal I|-\mathcal I}$ described above, while $L$ is the lower triangular Cholesky decomposition of $Q_{\mathcal I\mathcal  I}^{-1}$ for the three different cases, but we have to remember to multiply $L$ by $\sigma_u$.

### MCMC Blocking implementation

```{r MCMC-blocking}
#--------------------------------------------------------------------
get_Q <- function(n){
  #Function to create precision matrix that is not scaled
  library(Matrix) 
  m <- diag(2,n)
  m[abs(row(m) - col(m)) == 1] <- -1
  m[1,1] = 1
  m[n,n] = 1
	return(Matrix(m, sparse = F))
}

#--------------------------------------------------------------------

MCMC_block_sampler <- function(M, tau_0, nsteps = 50000, alpha = 2, beta = 0.05, verbose=F){
  # MCMC block Sampling algorithm for Tokyo rainfall dataset
  # M       : Length of block intervals
  # tau_0   : 366-length vector of initial tau values
  # alpha   : Shape parameter for sigma_u^2
  # beta    : Scale parameter for sigma_u^2
  # nsteps  : Number of MCMC iterations, defaults to 50.000

  start = proc.time()[3] # To measure time
  
  # Intializations
  n.rain = rain$n.rain
  n.years = rain$n.years
  accepted = 0
  
  current_tau = tau_0
  tau = matrix(0, nrow = length(current_tau), ncol = nsteps)
  sigmas = rep(0, nsteps)
  tau_proposed = matrix(0, nrow = length(current_tau))
  
  # Progress bar initialization
  pb = txtProgressBar(min = 0, max = nsteps, initial = 2)
  
  # Calculations for the blocks
  leftovers = ifelse(366%%M==0, M, 366%%M) # Leftovers after all "full" blocks
  nr_blocks = ceiling(366/M) # Number of blocks
  
  # Initializing matrices
  #-----------------------------------------------------------------------------
  Q = get_Q(366) #The full precision matrix, not scaled
  
  # First block matrices, a=1
  Q_AA1 = Q[1:M,1:M] 
  inv_Q_AA1 = solve(Q_AA1)
  mu_factor_1 = inv_Q_AA1[, M]
  
  # Midsection matrices, a != 1,  b != 366 
  Q_AA2 = Q[2:(M+1), 2:(M+1)]
  inv_Q_AA2 = solve(Q_AA2)
  mu_factor_2 = cbind(inv_Q_AA2[,1], inv_Q_AA2[, M])
  
  # Last block matrices, b=366
  Q_AA3 = Q[(367-M):366, (367-M):366]
  inv_Q_AA3 = solve(Q_AA3)
  mu_factor_3 = inv_Q_AA3[,1]
  
  
  # Cholesky decompositions of inverses of block matrices
  #     Transposed to get lower triangular
  chol_inv_Q_AA1 = t(chol(inv_Q_AA1)) 
  chol_inv_Q_AA2 = t(chol(inv_Q_AA2))
  chol_inv_Q_AA3 = t(chol(inv_Q_AA3))

  
  sigma_u = sqrt(1/rgamma(1, shape = alpha, rate = beta))
  
  # #For-loop with every iteration
  for (k in 1:nsteps) {
    if(verbose){setTxtProgressBar(pb,k)} # Update progress bar

    #MH-step
    #---------------------------------------------------------------------------
    
    #Pre-draw normal and uniform samples
    normal_samples = matrix(rnorm(nr_blocks*M), nrow=nr_blocks)
    uniform_samples = runif(nr_blocks)
    
    ##### First block, a = 1 #####
    a = 1; b = M
    tau_proposed = mu_factor_1*current_tau[b+1] + 
      sigma_u*chol_inv_Q_AA1%*%normal_samples[1,]
    
    # Calculate the acceptance probability for the proposed samples
    # We don't have to use min(1, .) since if larger than 1, we accept anyways
    acceptence_probability = exp(sum(n.rain[a:b]*(tau_proposed-tau[a:b])+
                                       n.years[a:b]*log((1+exp(tau[a:b]))/(
                                         1+exp(tau_proposed)))))
    # Check if we accept the proposed samples. If we do, update current tau
    if(uniform_samples[1] < acceptence_probability){
      current_tau[a:b] = tau_proposed
      accepted = accepted + M/366
    }
    
    
    ##### Midsection, a != 1, b != 366 #####
    for (j in 2:(nr_blocks-1)) {
      a = 1+(j-1)*M; b = j*M
      tau_proposed = as.array(mu_factor_2%*%current_tau[c(a-1,b+1)])+
        as.array(sigma_u*chol_inv_Q_AA2%*%normal_samples[j,])
      #Calculate the acceptance probability for the proposed samples
      acceptence_probability = exp(sum(n.rain[a:b]*(tau_proposed-current_tau[a:b])+n.years[a:b]*log((1+exp(current_tau[a:b]))/(1+exp(tau_proposed)))))
      #Check if we accept the proposed samples, if we do, update current tau
      if(uniform_samples[j] < acceptence_probability){
        current_tau[a:b] = tau_proposed
        accepted = accepted + M/366
      }
    }
    
    ##### Last block, b = 366 #####
    a = 366-M+1; b = 366; a_leftover = 366 - leftovers + 1
    lower_index = M - leftovers + 1 # Need this to make it work
    tau_proposed = mu_factor_3*current_tau[a-1]+
      sigma_u*chol_inv_Q_AA3%*%normal_samples[nr_blocks,]
    
    acceptence_probability = exp(sum(n.rain[a_leftover:b]*(
      tau_proposed[lower_index:M]-current_tau[a_leftover:b])+
        n.years[a_leftover:b]*log((1+exp(current_tau[a_leftover:b]))/(
          1+exp(tau_proposed[lower_index:M])))))
    
    if(uniform_samples[nr_blocks] < acceptence_probability){
      current_tau[a_leftover:b] = tau_proposed[lower_index:M]
      accepted = accepted + leftovers/366
    }

    
    tau[, k] = current_tau
    
    # Gibbs step
    #---------------------------------------------------------------------------
    sigmas[k] = 1/rgamma(10, shape = (alpha + (366-1)/2),
                              rate = beta + 0.5 * sum(diff(current_tau)^2))
    
    sigma_u = sqrt(sigmas[k])
    
    #---------------------------------------------------------------------------


  }

  close(pb)
  # Return tau samples, variance samples, runtime in seconds and acceptance rate
  res = list(
    tau        = tau,
    runtime    = proc.time()[3]-start,
    sigma_u    = sigmas,
    acceptance = accepted/(nsteps)
    )

  return(res)
}
```

Figure \@ref(fig:block-acceptance-plot) shows a plot of the acceptance rates as a function of $M$ for a total of 2000 iterations, and similarly, the plot below shows the runtime of the algorithm as a function of $M$. Both plots show a trade-off between acceptance rate and runtime. We would like to choose an $M$ with as high acceptance rate as possible while simultaneously giving low computational time. Increasing $M$ yields lower accept rates and lower runtime. From Figure \@ref(fig:block-acceptance-plot), we choose $M=10$, as it has a relatively low runtime while also preserving a high acceptance rate.

It is expected that the runtime will be lower for higher values of $M$, as there are fewer computations. However, the acceptance rate is also expected to diminish for increasing $M$, because the algorithm accepts or rejects entire blocks.

```{r find-optimal-m}
plot_acceptance_and_runtime <- function(verbose=F){
  M = c(1, 3, 5, 10, 15, 20, 50, 100)
  acceptance_list = c()
  runtime_list = c()
  steps = 2000
  
  ind_count = 1
  for(m in M){
    if(verbose){print(paste("Fitting for M", m, "..."))}
    fit = MCMC_block_sampler(m, rnorm(366, 0, sqrt(0.07)), nsteps = steps)
    acceptance_list[ind_count] = fit$acceptance
    runtime_list[ind_count] = fit$runtime
    ind_count = ind_count + 1
  }
  return(list(M=M, acceptance=acceptance_list, runtime=runtime_list))
}
M_plotting_list = plot_acceptance_and_runtime()
```

```{r block-runtime-plot, echo=F, fig.cap=r'(Plot of runtime over different $M$.)'}
par(mfrow=c(1,2))
plot(M_plotting_list$M, M_plotting_list$runtime, type="b", main="Runtime over M", xlab="M", ylab="Time taken (seconds)")
plot(M_plotting_list$M, M_plotting_list$acceptance, type="b", main="Acceptance rates over M", xlab="M", ylab="Acceptance rate")
```

We now apply the algorithm with 50 000 iterations and $M=10$.
```{r block-m10-results}
result_block = MCMC_block_sampler(10,rnorm(366, 0, sqrt(0.007)),
                                    nsteps = 50000)
```

This run had a runtime of `r round(result_block$runtime,3)` seconds with acceptance rate `r round(result_block$acceptance, 3)`. Although we expected the blocking algorithm to be faster than the single-site algorithm, it was not. This is mainly due to code optimizations such as the `as.array` convertions, which had to be called due to unexpected behavior. With additional time, one could get rid of such time-costly operations and achieve a faster blocking algorithm.

We produce the same plots as for the single-site MCMC, namely trace plots, histograms and autocorrelation plots. In addition, we compare the predictions from the single-site algorithm to our blocking algorithm.

The trace plots in Figures \@ref(fig:block-traceplots) and \@ref(fig:block-first1000-traceplots) follow the same trends as the single-site results. Thus, we choose the same burn-in period of 500.

```{r block-traceplots, echo=FALSE, fig.cap=r'(Trace plots of $\sigma_u^2$ and $\tau_t$ for $t=1, 201,366$ for all iterations of blocking MCMC.)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1.8, 1.5, 0.5))
traceplot(result_block)
traceplot(result_block, day=1)
traceplot(result_block, day=201)
traceplot(result_block, day=366)
```

```{r block-first1000-traceplots, echo=FALSE, fig.cap=r'(Trace plots of the same parameters for the 1000 first values with blocking MCMC.)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1.8, 1.5, 0.5))
plot_lim = c(0,1000)
traceplot(result_block, xlim=plot_lim)
traceplot(result_block, day=1, xlim=plot_lim)
traceplot(result_block, day=201, xlim=plot_lim)
traceplot(result_block, day=366, xlim=plot_lim)
```

The histograms, as shown in Figure \@ref(fig:block-histograms), also follow the same pattern as the single-site case, with corresponding means. This is also to be expected, since both algorithms are MCMC samplers.

```{r block-histograms, echo=FALSE, fig.cap=r'(Histograms of $\sigma_u^2$ and $\tau_t$ for $t=1, 201,366$ with their corresponding 95\% credible intervals - for blocking MCMC.)'}

p1 = plot_hist(result_block)
p2 = plot_hist(result_block, 1)
p3 = plot_hist(result_block, 201)
p4 = plot_hist(result_block, 366)
pdf(NULL) # Fixes blank page bug in ggpubr
res <- ggarrange(p1,p2,p3,p4, ncol=2, nrow=2, common.legend=TRUE, legend="top")
x <- dev.off() # Fixes blank page bug in ggpubr
res
```

The plots that differ significantly from the single-site case is however the autocorrelation plots in Figure \@ref(fig:block-acf). Here, we see a rapid decay of the autocorrelation for $\tau_1$ and $\tau_{366}$. $\tau_{206}$ does not seem to have any significant changes, and $\sigma_u^2$ seem to increase a bit compared to the autocorrelation of the single-site case. This can however be attributed to the monte carlo randomness.

The fact that the values are less correlated in the blocking algorithm, may be since the correlation effect between $\tau_i$ and $\tau_j$ is reduced when they are jointly updated in the same block. 

```{r block-acf, echo=FALSE, fig.cap=r'(Autocorrelation plot of the four parameters as a function of lag, for blocking MCMC)'}
par(mfrow = c(2,2))
par(mar = c(3.8, 1, 1, 2.1))
tau_no_burnin <- remove_burnin_tau(result_block$tau)
sigma_no_burnin <- remove_burnin_sigma(result_block$sigma_u)
acf(sigma_no_burnin)
mtext(TeX(r'($\sigma_u^2$)'), side=3, line=-3, outer=TRUE, adj=0.2)
acf(tau_no_burnin[1,])
mtext(TeX(r'($\tau_1$)'), side=3, line=-3, outer=TRUE, adj=0.75)
acf(tau_no_burnin[206,])
mtext(TeX(r'($\tau_{206}$)'), side=3, line=-16, outer=TRUE, adj=0.2)
acf(tau_no_burnin[366,])
mtext(TeX(r'($\tau_{366}$)'), side=3, line=-16, outer=TRUE, adj=0.75)
```

Lastly, we plot a comparison between the single-site results and the blocking results in Figure \@ref(fig:block-singlesite-comparison). They seem to coincide well as expected.

```{r block-singlesite-comparison, echo=FALSE, fig.cap=r'(Comparison between single-site and blocking MCMC. Here, M=10.)'}
compare_singlesite_vs_blocking <- function(result_singlesite,
                                           result_blocking,plotMean=TRUE){
  # Plots the probability of rain alongside the sampled values for each day.
  #pi_data = rain$n.rain/rain$n.years
  pi_mcmc_singlesite = if(plotMean) expit(rowMeans(tail(
    result_singlesite$tau[, ], n=49500)))
    else expit(tail(t(result$tau), n=1)[,])
  pi_mcmc_blocking = if(plotMean) expit(rowMeans(tail(result_blocking$tau[, ],
                                                      n=49500)))
  else expit(tail(t(result_blocking$tau), n=1)[,])
  
  ci_low_singleSite = c()
  ci_high_singleSite = c()
  
  ci_low_blocking = c()
  ci_high_blocking = c()
  
  for(i in 1:length(result_singlesite$tau[, 1])){
    ci_all_singleSite = ci(expit(tail(
      result_singlesite$tau[i,], n=49500)), ci=0.95)
    ci_low_singleSite[i] = ci_all_singleSite$CI_low
    ci_high_singleSite[i] = ci_all_singleSite$CI_high
    
    ci_all_blocking = ci(expit(tail(
      result_blocking$tau[i, ], n=49500)), ci=0.95)
    ci_low_blocking[i] = ci_all_blocking$CI_low
    ci_high_blocking[i] = ci_all_blocking$CI_high
    
    
  }
  plot(pi_mcmc_blocking, type="l", col="blue", ylim=c(0,1))
  lines(ci_low_blocking, type = "l", lty="dashed", col = "blue")
  lines(ci_high_blocking, type="l", lty="dashed", col = "blue")
  
  #Single site
  lines(pi_mcmc_singlesite, type="l", lty = "solid", col="red")
  lines(ci_low_singleSite, type="l", lty = "dashed",col="red")
  lines(ci_high_singleSite, type="l", lty = "dashed",col="red")
  legend('topright', legend=c("Single site", "Blocking"),
         col=c("red", "blue"), lty=1, cex=0.8)
}

compare_singlesite_vs_blocking(result, result_block)

```

## Analyzing rainfall dataset using INLA

INLA is an abbreviation for integrated nested Laplace approximation and a widely used package in R for Bayesian inference.
```{r, echo=FALSE}
#install.packages("INLA",repos=c(getOption("repos"),
#INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
load("rain.rda")
```

We will now attempt to get the same predictions using the INLA-library in R. Note that we also place a prior on $\sigma_u^2$ in the INLA-model, since this is what we have done in the MCMC sampling algorithm. The prior is written in the hyper-argument for the function `f()` in the `inla` function call, and with keyword `prec`, we use the *log precision*. Thus, instead of using inverse gamma, we will be using log-gamma distribution on the prior - with the same parameters for $\alpha$ and $\beta$ as before, i.e. 2 and 0.05 respectively. By including `-1` in the function call, we remove the intercept and thus we don't place any priors on the intercept. We also use random walk of order $1$ on the INLA model, since this is done in the MCMC algorithm. The model is computed below:

```{r modelfit-inla}
fit_and_plot <- function(control, intercept=FALSE, constr=FALSE,
                         compare_MCMC=FALSE){
  # Main method for fitting and plotting INLA models.
  #
  # control      : list containing parameters for inla.control
  # intercept    : bool determining if intercept should be included or not
  # constr       : whether or not sum-to-zero constraint should be imposed
  # compare_MCMC : bool determining if plot should focus on INLA result
  #                (with credible interval), or by comparing the MCMC 
  #                results withthe an INLA model.
  
  # Prepare plot title based on parameters/settings
  intercept_str = if(intercept) "with" else "without"
  constr_str = if(constr) "with" else "without"
  plot_text <- if(!compare_MCMC) paste("True values vs. INLA. strategy",
                     control$strategy, control$int.strategy, ",",
                     intercept_str, "intercept,\n",
                     constr_str, "sum-to-zero constraint") else
                       "INLA results vs. MCMC. INLA"
  # Fit INLA model
  time_start = proc.time()[3]
  mod <- inla(n.rain ~ -1*(1-intercept) +
                f(day, model="rw1", constr=constr, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)
                ))),
              data=rain, Ntrials=n.years, control.compute=list(config=TRUE),
              family="binomial", verbose=FALSE, control.inla= control)
  runtime = proc.time()[3] - time_start
  
  # Plotting
  plot(rain$n.rain/rain$n.years, type="l", col="grey", xlab="Day",
       ylab="Predictions", main=paste(plot_text, ". Runtime:",
         round(runtime,3), "seconds"), cex.main=0.9, font.main = 1
       )
  
  lines(mod$summary.fitted.values$mean, type="l", col="red")
  legend_txt = c("True observations", "Predicted mean", "95% credible interval")
  if(compare_MCMC){
    pi_mcmc = expit(rowMeans(remove_burnin_tau(result$tau)))
    lines(pi_mcmc, type="l", col="blue")
    legend_txt[3] = "MCMC predictions"
  } 
  else{
    lines(mod$summary.fitted.values$`0.025quant`, lty="dashed", col="blue")
    lines(mod$summary.fitted.values$`0.975quant`, lty="dashed", col="blue")
  }
  legend("topright", legend=legend_txt, col=c("grey", "red", "blue"),
         lty=1, cex=0.8)
  return(runtime)
}
```

We can compare the predicted values, stored in `mod$summary.fitted.values`, to their true observations for each day. We plot the predicted mean with a 95% credible interval in Figure \@ref(fig:inla-comparison).

```{r inla-comparison, echo=F, fig.cap=r'(Predicted values from INLA model compared with a 95% credible interval plotted alongside the true observations. We observe that the predicted mean closely follows the true observations, and the true observations are rarely outside the 95% credible interval.)'}
runtime <- fit_and_plot(control = list(strategy="simplified.laplace",
                                       int.strategy="ccd"))
```
Note also that the runtime is `r round(runtime,3)`, which is significantly faster than any MCMC algorithm. The results also look similar to that of the MCMC predictions. We expect them to look practically identical since both use the same random walk model and the same prior on $\sigma_u^2$. This can be confirmed by comparing the results as done in Figure \@ref(fig:inla-vs-mcmc), where we use a single-site MCMC result to compare. We only use single-site since the blocking algorithm produces virtually the same plots. Although the predictions are very similar for INLA and MCMC, they are not exactly the same. This could be the result of the randomness, or Monte Carlo error, introduced in the MCMC algorithm.

```{r inla-vs-mcmc, echo=F, fig.cap=r'(Comparison of the results from INLA and the two MCMC methods)'}
fit_and_plot(control = list(strategy="simplified.laplace", int.strategy="ccd"), compare_MCMC=TRUE)
```


## INLA for different control settings

Another part of the INLA model used, is the `control.inla` parameter, where we specify the optimization strategy and integration strategy. For the optimization strategy, we use `simplified.laplace`, which is the default parameter and works well for most cases. It's based on a series expansion of the standard Laplace approximation around $\mu_i(\theta)$.

For the integration strategy, we use`ccd`, which is an abbreviation for a *central composite design*. This improves performance compared to integrating over an entire equidistant grid, since CDD uses fewer, well-placed points. CCD is however less accurate than a grid approach, so it is only beneficial to use when the dimensions are too large to fit a model using a grid within reasonable runtime. However, since our dimensions are $1$, we could get use of using the grid approach instead.

The results from the grid approach compared to the CCD approach is shown in Figure \@ref(fig:INLA-grid-int-control) for completeness, but is identical to `cdd`. In addition, we also plot a comparison between `laplace` and `simplified.laplace` in Figure \@ref(fig:INLA-grid-int-control), which also seems to be identical. This indicates that the results are robust to the choice of control settings.

```{r INLA-grid-int-control, fig.cap=r'(Plot comparing predictions using simplified laplace with CCD strategy as red, and grid strategy as blue to the left Similarly, the plot to the right compares using Laplace and simplified laplace strategy. Both plots seem to coincide.)'}

compare_inla_models <- function(control1, control2, main){
  # Helper to compare two INLA models
  mod_1 <- inla(n.rain ~ -1 +
                f(day, model="rw1", constr=FALSE, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control1
              )
  mod_2 <- inla(n.rain ~ -1 +
                f(day, model="rw1", constr=FALSE, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control2
              )
  plot(mod_1$summary.fitted.values$mean, type="l", col="red", main=main,
       xlab="Days", ylab="Predicted mean")
  lines(mod_2$summary.fitted.values$mean, type="l", lty="dashed", col="blue")
  legend("topright", col=c("red", "blue"), lty=1, legend=c(
    paste(control1$strategy, control1$int.strategy),
    paste(control2$strategy, control2$int.strategy)))
}
par(mfrow=c(1,2))
compare_inla_models(list(strategy="simplified.laplace", int.strategy="grid"),
                    list(strategy="simplified.laplace", int.strategy="ccd"),
                    main="CCD vs Grid method"
                    )
compare_inla_models(list(strategy="laplace", int.strategy="grid"),
                    list(strategy="simplified.laplace", int.strategy="grid"),
                    main="Simplified Laplace vs. Laplace"
                    )
```

## Intercept and constraints on INLA model
The models shown so far did not include any intercept, nor did they impose any sum-to-zero constraint. We will now explore how this may change our predictions.

Instead of modeling $\pi(\tau_t)$, by introducing an intercept term, we introduce the term $\pi(\eta_t)$ in which $\eta_t := \tau_t + \beta_0$, where $\beta_0$ is the intercept. In addition, we impose a sum-to-zero constraint, meaning that $\sum_t \tau_t = 0$.

We can start by plotting the mean of the fitted values for all variations, i.e. without intercept nor constraint, with intercept and constraint, without intercept with constraint and without intercept, with constraint. The four plots are shown in the resulting Figure \@ref(fig:inla-constr-intercept-compare). In summary, having an intercept term but also imposing a sum-to-zero constraint seem to be exactly the same as having neither an intercept, nor the constraint. If we don't impose the constraint on a model with an intercept, we will see slight deviations from the other two models. Lastly, if we impose the constraint on a model *without* any intercept, we get very large deviations, but the fitted values seem to be scaled along the y-axis. Looking at the y-axis, it makes sense that the last model is shifted upwards, in order to attain the sum-to-zero constraint.

```{r inla-constr-intercept-compare, echo=FALSE, fig.cap=r'(Comparison between models varying on intercept and sum-to-zero constraints.)'}
compare_inla_models_2 <- function(){
  # Helper to compare two INLA models
  control = list(strategy="simplified.laplace", int.strategy="ccd")
  
  # First model without intercept, without constraint
  mod_1 <- inla(n.rain ~ -1 +
                f(day, model="rw1", constr=F, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control
              )
  # Model with intercept and with constraint
  mod_2 <- inla(n.rain ~ f(day, model="rw1", constr=T, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control
              )
  # Model WITHOUT intercept, with constraint
  mod_3 <- inla(n.rain ~ -1 + f(day, model="rw1", constr=T, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control
              )
  # Model WITH intercept, WITHOUT constrinat
  mod_4 <- inla(n.rain ~ f(day, model="rw1", constr=F, hyper=list(prec=list(
                  prior="loggamma",
                  param=c(2,0.05)))),
              data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
              family="binomial", verbose=FALSE, control.inla=control
              )
  
  plot(mod_1$summary.linear.predictor$mean, type="l", col="red", main="Comparison of INLA models for varying intercepts and/or constraints.",
       xlab="Days", ylab="Mean of linear predictor")
  lines(mod_2$summary.linear.predictor$mean, type="l", lty="dashed", col="blue")
  lines(mod_3$summary.linear.predictor$mean, type="l", lty="dashed", col="green")
  lines(mod_4$summary.linear.predictor$mean, type="l", lty="dashed", col="orange")
  
  legend("bottomright", col=c("red", "blue", "green", "orange"), lty=c("solid", "dashed", "dashed", "dashed"), bg="white",
         legend=c("Without intercept without constraint",
                  "With intercept with constraint",
                  "Without intercept with constraint",
                  "With intercept without constraint")
  )
  
  return(list(Constr=mod_2$summary.fixed$sd, NoConstr=mod_4$summary.fixed$sd))
}
intercept_models = compare_inla_models_2()
```
